{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee1d2f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from platform import python_version\n",
    "from keras.models import Sequential\n",
    "from wandb.keras import WandbCallback\n",
    "from keras.layers import Dense, Reshape\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dropout, PReLU, ELU\n",
    "from tensorflow.keras.layers import Concatenate,ZeroPadding2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.layers import Multiply, add, BatchNormalization, UpSampling2D\n",
    "from tensorflow.keras.layers import Input, Lambda, Activation, Add, multiply, add, concatenate\n",
    "from keras.layers import Input, Conv2D, LeakyReLU, MaxPooling2D, Conv2DTranspose, Flatten, Dense, Reshape\n",
    "\n",
    "# Input Parameters\n",
    "# =============================================================================\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--loss_name', type=str, default='MAE', help='Loss name')\n",
    "parser.add_argument('--loss', type=str, default=None, help='loss')\n",
    "parser.add_argument('--bs', type=int, default=16, help='Batch size')\n",
    "parser.add_argument('--spe', type=int, default=500, help='Steps per epoch')\n",
    "args = parser.parse_args()\n",
    "\n",
    "loss_name =args.loss_name\n",
    "batch_size = args.bs \n",
    "steps_per_epoch = args.spe\n",
    "epoch_num = 100\n",
    "learning_rate_value = 0.0001\n",
    "activation = 'lrelu' # lrelu, prelu, elu\n",
    "version_name = f'Model_{batch_size}-{steps_per_epoch}-{epoch_num}-{activation}-{loss_name}'\n",
    "model_version = f'Unet_{batch_size}-{steps_per_epoch}-{epoch_num}-{activation}-{loss_name}'\n",
    "# =============================================================================\n",
    "    \n",
    "def preprocessing_function(log_data, params=None):\n",
    "    log_data = np.clip(log_data, a_min=-20, a_max=None)\n",
    "    min_value = -20\n",
    "    max_value = 0\n",
    "    min_max_scaled_data = (log_data - min_value) / (max_value - min_value)*2 -1\n",
    "    min_max_scaled_data = np.nan_to_num(min_max_scaled_data, 0)\n",
    "    return min_max_scaled_data\n",
    "\n",
    "\n",
    "def load_array_3d(filename):\n",
    "    \"\"\"Load numpy array\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        arr = np.load(f)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def data_norm(name, output = False, save_idx=None):\n",
    "    \"\"\"Load one data example and use log transform\n",
    "    Return array of size (Layers, Molecules, 1)\"\"\"\n",
    "    arr = load_array_3d(name) # arr shape is (111, 100) or (molecules, layers)\n",
    "    if output :\n",
    "        arr = arr[-1]\n",
    "    if save_idx is not None:\n",
    "        arr = arr[save_idx,:]\n",
    "    arr = np.log10(arr)[:,:64] # logarithm \n",
    "    arr = arr.reshape((64,64,1))\n",
    "    arr = preprocessing_function(arr) # logarithm \n",
    "    return arr\n",
    "\n",
    "\n",
    "def load_data(data_dir, output=False, save_idx=None):\n",
    "    \"\"\"Load all data (Train/Test) and apply log transform\n",
    "    Return it as np array of size (number_of_examples, Layers, Molecules,1)\"\"\"\n",
    "    data_list = np.sort(glob.glob(f'{data_dir}/*npy'))\n",
    "    array = []\n",
    "    _ = [array.append(data_norm(name, output=output, save_idx=save_idx)) for name in data_list]\n",
    "    array = np.array(array)\n",
    "    return array, data_list\n",
    "\n",
    "\n",
    "def split_data(input_data, output_data):\n",
    "    \"\"\"Split dataset into train/test/validation sets, assuming 80/10/10 split\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data,\n",
    "                                                        test_size=0.2, random_state=42)    \n",
    "    X_test, X_val, y_test, y_val  = train_test_split(X_test, y_test, test_size=0.5,random_state=42) \n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "def load_names(X_dir):\n",
    "    data_list_X = np.sort(glob.glob(f'{X_dir}/*npy'))\n",
    "    file_names = [path.split('/')[-1] for path in data_list_X]\n",
    "    X_train, X_test, X_val, _, _, _ = split_data(file_names, file_names)\n",
    "    return X_train, X_test, X_val\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, filenames, batch_size, save_idx, configuration):\n",
    "        self.filenames = filenames\n",
    "        self.configuration = configuration\n",
    "        self.batch_size = batch_size\n",
    "        self.save_idx = save_idx\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.filenames)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_filenames = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_X = []\n",
    "        batch_y = []\n",
    "        batch_info =[]\n",
    "        for filename in batch_filenames:\n",
    "            input_data = data_norm(f'/../{filename}',  save_idx = self.save_idx)\n",
    "            output_data = data_norm(f'/../{filename}', output=True, save_idx = self.save_idx)  # Update with your output data\n",
    "            additional_info = self.configuration[self.configuration['System_ID'].isin([filename.split('.')[0]])]\n",
    "            additional_info = list(additional_info[['planet_radius', 'planet_mass', 'co_ratio','isothermal_T', 'metalicity']].to_numpy()[0])\n",
    "            batch_X.append(input_data)\n",
    "            batch_y.append(output_data)\n",
    "            batch_info.append(additional_info)\n",
    "        batch_X = np.array(batch_X)\n",
    "        batch_y = np.array(batch_y)\n",
    "        batch_info = np.array(batch_info)\n",
    "        return [batch_X,batch_info],  batch_y\n",
    "\n",
    "# Network\n",
    "# =============================================================================\n",
    "def activation_function(conv, act_type='lrelu', name='activation'):\n",
    "    if act_type == 'lrelu':\n",
    "        result = LeakyReLU(alpha=0.2,name=f'{act_type}_{name}')(conv)\n",
    "        return result\n",
    "        \n",
    "    if act_type == 'prelu':\n",
    "        result = PReLU(name=f'{act_type}_{name}')(conv)\n",
    "        return result\n",
    "        \n",
    "    if act_type == 'elu':\n",
    "        result = ELU(name=f'{act_type}_{name}')(conv)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "def model_additional_info(input_dim = 5, output_shape = (8, 8, 1)):\n",
    "    \"\"\"\n",
    "    Creates a neural network model that takes a vector of specified input dimensions\n",
    "    and outputs a map of specified shape.\n",
    "    \n",
    "    Parameters:\n",
    "    input_dim (int): Number of variables in the input vector.\n",
    "    output_shape (tuple): Dimensions of the output map.\n",
    "\n",
    "    Returns:\n",
    "    model: A Keras Sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='linear', input_dim=input_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(64, activation='linear'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(output_shape[0] * output_shape[1], activation='linear'))\n",
    "    model.add(LeakyReLU(alpha=0.2)) \n",
    "    model.add(Reshape(output_shape))\n",
    "    return model\n",
    "\n",
    "\n",
    "def convolution_module(input_tensor, filters,num, drop_rate = 0.2):\n",
    "    conv = Conv2D(filters, (3, 3), padding='same', name = f'convolution_{num}_2')(input_tensor)\n",
    "    conv = LeakyReLU(alpha=0.2, name=f'lrelu_{num}')(conv)\n",
    "    conv = Conv2D(filters, (3, 3), padding='same', name = f'convolution_{num}_2')(conv)\n",
    "    conv = LeakyReLU(alpha=0.2, name=f'lrelu_{num}_2')(conv)\n",
    "    pool = MaxPooling2D((2, 2), padding='same', name =f'pool{num}')(conv)\n",
    "    return pool, conv\n",
    "\n",
    "\n",
    "def AttnBlock2D(x, g, desired_dimensionality, num):\n",
    "    x_shape = x.shape\n",
    "    g_shape = g.shape\n",
    "    xl = Conv2D(desired_dimensionality,(1,1),strides=(2,2),\n",
    "                activation=\"relu\",padding = \"same\", name=f'Attention_conv_x_{num}')(x)\n",
    "    gl = Conv2D(desired_dimensionality,(1,1),\n",
    "                activation=\"relu\",padding = \"same\", name=f'Attention_conv_g_{num}')(g)\n",
    "    xg = Add(name=f'Attention_add_{num}')([xl,gl])\n",
    "    xg = Activation(\"relu\", name=f'Attention_relu_{num}')(xg)\n",
    "    xg = Conv2D(1,(1,1),activation=\"sigmoid\",padding = \"same\", name=f'Attention_conv_{num}')(xg)\n",
    "    xg_shape = xg.shape\n",
    "    xg = UpSampling2D((x_shape[1]//xg_shape[1],x_shape[2]//xg_shape[2]), \n",
    "                                      name=f'Attention_up_{num}')(xg)\n",
    "    output = Multiply(name=f'Attention_mult_{num}')([xg,x])\n",
    "    return output\n",
    "\n",
    "\n",
    "def attention_upsample_and_concat(x1, x2, output_channels, in_channels, filters,num,drop_rate = 0.2):\n",
    "    pool_size = 2\n",
    "    x2 = AttnBlock2D(x2, x1, in_channels, num)\n",
    "    upsampled = Conv2DTranspose(output_channels, (pool_size, pool_size),\n",
    "                                strides=(pool_size, pool_size), padding='same', name=f'upsample_{num}')(x1)\n",
    "    concat = Concatenate(axis=3)([upsampled, x2])\n",
    "    conv = Conv2D(filters, (3, 3), padding='same', name =f'convolution_{num}')(concat)\n",
    "    conv = activation_function(conv, act_type=activation, name=f'{num}_up') \n",
    "    \n",
    "    conv = Conv2D(filters, (3, 3), padding='same',name=f'convolution_{num}_2' )(conv)\n",
    "    conv = activation_function(conv, act_type=activation,name= f'{num}_2_up')  \n",
    "    return conv\n",
    "\n",
    "\n",
    "def network(input, additional_input, additional_output_shape = (8, 8, 1)):\n",
    "    pool1, conv1 = convolution_module(input, 32,1)\n",
    "    pool2, conv2 = convolution_module(pool1, 64,2)\n",
    "    pool3, conv3 = convolution_module(pool2, 128,3)\n",
    "    pool4, conv4 = convolution_module(pool3, 256,4)\n",
    "\n",
    "    additional_model = model_additional_info(output_shape = additional_output_shape)\n",
    "    additional_output = additional_model(additional_input)\n",
    "    \n",
    "    combined = concatenate([conv4, additional_output])\n",
    "    \n",
    "    up7 = attention_upsample_and_concat(combined, conv3, 128, 257,128,7)\n",
    "    up8 = attention_upsample_and_concat(up7, conv2, 64, 128,64,8)\n",
    "    up9 = attention_upsample_and_concat(up8, conv1, 32, 64,32,9)\n",
    "    conv10 = Conv2D(1, (1, 1), padding='same', name = 'convolution_10')(up9)\n",
    "    return conv10\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "# =============================================================================\n",
    "\n",
    "def get_updated_M_for_epoch(current_epoch, N_start = 10, N_end = 20,\n",
    "                            M_start = 10.0, M_end = 1.5):\n",
    "    #N_start = 10\n",
    "    #N_end = 20\n",
    "    #M_start = 5.0\n",
    "    #M_end = 2.0\n",
    "\n",
    "    # Check if current_epoch is less than N_start\n",
    "    if current_epoch < N_start:\n",
    "        return M_start\n",
    "\n",
    "    # Check if current_epoch is greater than N_end\n",
    "    elif current_epoch > N_end:\n",
    "        return M_end\n",
    "    else:\n",
    "        # Calculate the step change for M\n",
    "        M_step = (M_start - M_end) / float(N_end - N_start)\n",
    "        # Calculate the updated M for the current epoch\n",
    "        M = M_start - (float(current_epoch) - N_start) * M_step\n",
    "        M = tf.maximum(M, M_end)  # Ensure M does not go below M_end\n",
    "        return float(M)\n",
    "\n",
    "\n",
    "class WandbCustomCallback(Callback):\n",
    "    def __init__(self, val_generator, val_steps, molecules):\n",
    "        super().__init__()\n",
    "        self.val_generator = val_generator\n",
    "        self.val_steps = val_steps  # This will be 1 if you want to use one batch\n",
    "        self.molecules = molecules\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Collect predictions and true labels from the validation generator\n",
    "        # Since val_steps is 1, we only take one batch\n",
    "        val_x, val_y = next(iter(self.val_generator))\n",
    "        val_pred = self.model.predict(val_x)\n",
    "\n",
    "        # Log the plots using the collected data\n",
    "        self.log_prediction_plots(val_x,val_y, val_pred)\n",
    "        self.log_prediction_plots_molecules(val_x,val_y, val_pred, self.molecules)\n",
    "        self.height_abundance_plot(val_x,val_y, val_pred, self.molecules)\n",
    "\n",
    "    def log_prediction_plots(self,val_x, true_data, pred_data):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "       \n",
    "        ax.plot(true_data[:4,:,:].flatten(), pred_data[:4,:,:].flatten(), '.', color='royalblue')\n",
    "        ax.plot([-1, 1], [-1, 1], '-', color='Black')\n",
    "        plt.xlabel('Ground truth')\n",
    "        plt.ylabel('Prediction')\n",
    "        plt.title('All layers and molecules in 4 validation atmospheres')\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        wandb.log({\"Predicted vs True - 4 Atmospheres\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def log_prediction_plots_molecules(self,val_x, true_data, pred_data, molecules):\n",
    "        molecules_list = [40, 43, 6, 45, 38, 13, 35, 53, 31, 46]\n",
    "        for mol in molecules_list:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "            ax.plot(true_data[:4, mol, :].flatten(), pred_data[:4, mol, :].flatten(), '.', color='royalblue')\n",
    "            ax.plot([-1, 1], [-1, 1], '-', color='Black')\n",
    "            \n",
    "            plt.xlabel('Ground truth')\n",
    "            plt.ylabel('Prediction')\n",
    "            plt.title(f'All layers for {molecules[mol]} in 4 validation atmospheres')\n",
    "            plt.grid()\n",
    "            plt.tight_layout()\n",
    "            wandb.log({f\"Predicted vs True - Molecule {molecules[mol]}\": wandb.Image(plt)})\n",
    "            plt.close()\n",
    "\n",
    "    def height_abundance_plot(self,val_x, true_data, pred_data, molecules):\n",
    "        molecules_list = ['CH3', 'CH4', 'O2', 'CO', 'CO2', 'C2H5', 'C2H2', 'C2H4', 'H2O']\n",
    "        layers = np.arange(0, 64)\n",
    "        colors = [\"#e6194B\", \"#3cb44b\", \"#ffe119\", \"#4363d8\", \"#f58231\", \n",
    "                  \"#911eb4\", \"#42d4f4\", \"#f032e6\", \"#bfef45\"]\n",
    "        for chosen_atm in [0, 1, 2, 3]:\n",
    "            fig, ax = plt.subplots(figsize=(9, 4.8))\n",
    "            for i, mol in enumerate(molecules_list):\n",
    "                color = colors[i % len(colors)]\n",
    "                \n",
    "                ax.plot(self.inverse_preprocessing(true_data[chosen_atm, molecules.index(mol), :]),\n",
    "                        layers, '-', label=mol, color=color)\n",
    "                ax.plot(self.inverse_preprocessing(pred_data[chosen_atm, molecules.index(mol), :]),\n",
    "                        layers, '--', label=f'{mol} P', color=color)\n",
    "            plt.grid()\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.title(f'Predicted (P, dashed line) vs Ground Truth (full line) abundances, atm {chosen_atm}')\n",
    "            plt.xlabel('Abundance')\n",
    "            plt.ylabel('Layer')\n",
    "            plt.tight_layout()\n",
    "            wandb.log({f\"Abundance vs Layer - Atmosphere {chosen_atm}\": wandb.Image(plt)})\n",
    "            plt.close()\n",
    "\n",
    "    def inverse_preprocessing(self, scaled_data):\n",
    "        min_value = -20\n",
    "        max_value = 0\n",
    "        log_data = ((scaled_data + 1) / 2) * (max_value - min_value) + min_value\n",
    "        return log_data\n",
    "\n",
    "\n",
    "class DynamicMAELoss(tf.keras.losses.Loss):\n",
    "    def __init__(self,output_shape, N=1, threshold=0.015, pixel_positions=None, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.N = N\n",
    "        self.threshold = threshold\n",
    "        self.pixel_positions = tf.constant(pixel_positions, dtype=tf.int32) if pixel_positions is not None else None\n",
    "        self.dynamic_weights = tf.Variable(tf.fill([1,64,64,1], 1.0), dtype=tf.float32, trainable=False) \n",
    "        self.current_epoch = tf.Variable(0, trainable=False, dtype=tf.int32)\n",
    "\n",
    "\n",
    "    def update_dynamic_weights(self, weights):\n",
    "        self.dynamic_weights.assign(weights)\n",
    "\n",
    "    def weighted_mae(self, y_true, y_pred, weights):\n",
    "        return tf.reduce_mean(tf.abs(y_true - y_pred) * weights) \n",
    "\n",
    "    def increment_epoch(self):\n",
    "        self.current_epoch.assign_add(1)\n",
    "\n",
    "\n",
    "    def call(self, y_true, y_pred):\n",
    "        if self.current_epoch <= self.N:            \n",
    "            return tf.reduce_mean(tf.abs(y_true - y_pred)) \n",
    "            weights = tf.fill(tf.shape(y_true), 1.0)\n",
    "            if self.dynamic_weights is not None:\n",
    "                dynamic_mask = tf.greater(self.dynamic_weights, self.threshold)\n",
    "                dynamic_mask = tf.broadcast_to(dynamic_mask, tf.shape(weights)) \n",
    "                weights = tf.where(dynamic_mask, 1.2, weights)\n",
    "\n",
    "                if self.pixel_positions is not None:\n",
    "                    for pos in self.pixel_positions:\n",
    "                        indices = tf.stack([tf.range(tf.shape(y_true)[0]),\n",
    "                                            tf.fill([tf.shape(y_true)[0]], pos[0]),\n",
    "                                            tf.fill([tf.shape(y_true)[0]], pos[1])],\n",
    "                                            axis=1)\n",
    "                        updated_M = get_updated_M_for_epoch(self.current_epoch.value())\n",
    "                        updates = tf.fill([tf.shape(indices)[0], 1], updated_M)\n",
    "                        weights = tf.tensor_scatter_nd_update(weights, indices, updates)\n",
    "        return self.weighted_mae(y_true, y_pred, weights)\n",
    "\n",
    "\n",
    "class UpdateDynamicWeightsCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, loss_instance, val_generator, N):\n",
    "        super().__init__()\n",
    "        self.loss_instance = loss_instance\n",
    "        self.val_generator = val_generator\n",
    "        self.N = N\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        # Update the current epoch attribute in the loss instance\n",
    "        gc.collect()\n",
    "        self.loss_instance.increment_epoch()\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if self.loss_instance.current_epoch >= self.N:\n",
    "            total_loss = 0\n",
    "            batches = 0\n",
    "            for x, y in self.val_generator:\n",
    "                val_preds = self.model.predict(x, verbose = 0)\n",
    "                total_loss += np.mean(np.abs(val_preds - y), axis=0)\n",
    "                batches += 1\n",
    "                if batches >=160: # len(self.val_generator):\n",
    "                    break\n",
    "            average_val_loss = total_loss / batches\n",
    "            average_val_loss = np.reshape(average_val_loss, (1, 64, 64, 1))\n",
    "            self.loss_instance.update_dynamic_weights(average_val_loss)\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Data Loading\n",
    "# =============================================================================\n",
    "\n",
    "configuration = pd.read_csv('../data/configuration/system_configuration.csv')\n",
    "\n",
    "par_min = configuration['co_ratio'].min()\n",
    "par_max = configuration['co_ratio'].max()\n",
    "configuration['co_ratio']=(configuration['co_ratio']-par_min)/(par_max-par_min) * 2 - 1\n",
    "\n",
    "par_min = 0.3370963034022992 \n",
    "par_max = 3.876389678499647\n",
    "configuration['planet_radius']=(configuration['planet_radius']-par_min)/(par_max-par_min) * 2 - 1\n",
    "\n",
    "par_min = 0.3\n",
    "par_max = 10\n",
    "configuration['planet_mass']=(configuration['planet_mass']-par_min)/(par_max-par_min) * 2 - 1\n",
    "\n",
    "par_min = 1100\n",
    "par_max = 2000\n",
    "configuration['isothermal_T']=(configuration['isothermal_T']-par_min)/(par_max-par_min) * 2 - 1\n",
    "\n",
    "par_min = 0.5\n",
    "par_max = 100\n",
    "configuration['metalicity']=(configuration['metalicity']-par_min)/(par_max-par_min) * 2 - 1\n",
    "\n",
    "save_molecules = np.array([ 34,  36,  39,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,\n",
    "        51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,\n",
    "        64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n",
    "        77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  90,  91,\n",
    "        94,  96,  97, 100, 101, 102, 104, 105, 106, 108, 109, 110])\n",
    "\n",
    "molecule_len = len(save_molecules)\n",
    "\n",
    "molecules= [\"HCNO\",\"N2O\",\"CH2CHO\",\"CH3CO\",\"NCO\",\"CH3O\",\"O2\",\"CH3CHO\",\"HNO\",\"C\",\n",
    "            \"CHCO\",\"CO2H\",\"HOCN\",\"C2H5\",\"C2H\",\"CH2OH\",\"CH\",\"C2H6\",\"C2H3\",\"CH2CO\",\"NNH\",\n",
    "            \"H2CN\",\"CH3OH\",\"N4S\",\"N2D\",\"CN\",\"1CH2\",\"HNCO\",\"NO\",\"O3P\",\"O1D\",\"C2H4\",\"NH\",\n",
    "            \"3CH2\",\"HCO\",\"C2H2\",\"H2CO\",\"NH2\",\"CO2\",\"OH\",\"CH3\",\"HCN\",\"NH3\",\"CH4\",\"N2\",\n",
    "            \"CO\",\"H2O\",\"H\",\"He\",\"H2\",\"N2H2\",\"N2H3\",\"HNOH\",\"NH2OH\",\"H2NO\",\"C2N2\",\"HCNH\",\n",
    "            \"HNC\",\"NCN\",\"HCOH\",\"HOCHO\",\"H2Oc\",\"CH4c\",\"NH3c\"]\n",
    "\n",
    "X_train, X_test, X_val = load_names('/../data/ode_results_time_steps/')\n",
    "train_generator = DataGenerator(X_train, batch_size, save_molecules,configuration)\n",
    "test_generator = DataGenerator(X_test , batch_size, save_molecules,configuration)\n",
    "val_generator = DataGenerator(X_val, batch_size, save_molecules,configuration)\n",
    "\n",
    "# =============================================================================\n",
    "            \n",
    "\n",
    "input_shape = (64, 64, 1)\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "additional_input_shape = (5,)\n",
    "additional_input = tf.keras.Input(shape=additional_input_shape)\n",
    "\n",
    "output = network(inputs,additional_input)\n",
    "model = Model(inputs=[inputs,additional_input], outputs=output)\n",
    "\n",
    "positions = [...]\n",
    "\n",
    "output_shape_loss = [16, 64, 64, 1]\n",
    "wandb_custom_callback = WandbCustomCallback(val_generator, 2, molecules)\n",
    "dynamic_loss = DynamicMAELoss(output_shape=output_shape_loss, N=0, threshold=0.009, pixel_positions=positions)\n",
    "update_epoch_callback = UpdateDynamicWeightsCallback(dynamic_loss, val_generator, N=0)\n",
    "\n",
    "callbacks = [WandbMetricsLogger(), wandb_custom_callback,\n",
    "    ModelCheckpoint(filepath=f'{version_name}/{model_version}.h5', monitor='val_loss',\n",
    "                    save_best_only=True),\n",
    "    TensorBoard(log_dir='logs'),\n",
    "    EarlyStopping(monitor='val_loss', patience=7),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.2*0.2*learning_rate_value),\n",
    "    CSVLogger(f'{version_name}/training_log.csv'),\n",
    "    update_epoch_callback]\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate_value), loss=dynamic_loss )\n",
    "model.fit(train_generator, epochs=epoch_num, validation_data=val_generator,callbacks=callbacks, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbb5ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (64, 64, 1)\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "additional_input_shape = (5,)\n",
    "additional_input = tf.keras.Input(shape=additional_input_shape)\n",
    "\n",
    "output = network(inputs,additional_input)\n",
    "model = Model(inputs=[inputs,additional_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d47753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
