{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee1d2f0",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../data/configuration/system_configuration.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 312\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m log_data\n\u001b[1;32m    306\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[1;32m    307\u001b[0m \n\u001b[1;32m    308\u001b[0m \n\u001b[1;32m    309\u001b[0m \u001b[38;5;66;03m# Data Loading\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;66;03m# =============================================================================\u001b[39;00m\n\u001b[0;32m--> 312\u001b[0m configuration \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../data/configuration/system_configuration.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    314\u001b[0m par_min \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.4\u001b[39m\n\u001b[1;32m    315\u001b[0m par_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.5\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/pandas/io/parsers/readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/miniforge3/envs/tf/lib/python3.10/site-packages/pandas/io/common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    855\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    856\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    857\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    858\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 859\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    867\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    868\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/configuration/system_configuration.csv'"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "from platform import python_version\n",
    "from keras.models import Sequential\n",
    "from wandb.keras import WandbCallback\n",
    "from keras.layers import Dense, Reshape\n",
    "from wandb.keras import WandbMetricsLogger\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.layers import Dropout, PReLU, ELU\n",
    "from tensorflow.keras.layers import Concatenate,ZeroPadding2D\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from tensorflow.keras.callbacks import TensorBoard, ReduceLROnPlateau, CSVLogger\n",
    "from tensorflow.keras.layers import Multiply, add, BatchNormalization, UpSampling2D\n",
    "from tensorflow.keras.layers import Input, Lambda, Activation, Add, multiply, add, concatenate\n",
    "from keras.layers import Input, Conv2D, LeakyReLU, MaxPooling2D, Conv2DTranspose, Flatten, Dense, Reshape\n",
    "\n",
    "# Input Parameters\n",
    "# =============================================================================\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--loss_name', type=str, default='MAE', help='Loss name')\n",
    "parser.add_argument('--loss', type=str, default=None, help='loss')\n",
    "parser.add_argument('--bs', type=int, default=16, help='Batch size')\n",
    "parser.add_argument('--spe', type=int, default=500, help='Steps per epoch')\n",
    "args = parser.parse_args()\n",
    "\n",
    "loss_name =args.loss_name\n",
    "batch_size = args.bs \n",
    "steps_per_epoch = args.spe\n",
    "epoch_num = 100\n",
    "learning_rate_value = 0.0001\n",
    "activation = 'lrelu' # lrelu, prelu, elu\n",
    "version_name = f'Model_{batch_size}-{steps_per_epoch}-{epoch_num}-{activation}-{loss_name}'\n",
    "model_version = f'Unet_{batch_size}-{steps_per_epoch}-{epoch_num}-{activation}-{loss_name}'\n",
    "# =============================================================================\n",
    "    \n",
    "def preprocessing_function(log_data, params=None):\n",
    "    log_data = np.clip(log_data, a_min=-20, a_max=None)\n",
    "    min_value = -20\n",
    "    max_value = 0\n",
    "    min_max_scaled_data = (log_data - min_value) / (max_value - min_value)*2 -1\n",
    "    min_max_scaled_data = np.nan_to_num(min_max_scaled_data, 0)\n",
    "    return min_max_scaled_data\n",
    "\n",
    "\n",
    "def load_array_3d(filename):\n",
    "    \"\"\"Load numpy array\"\"\"\n",
    "    with open(filename, 'rb') as f:\n",
    "        arr = np.load(f)\n",
    "    return arr\n",
    "\n",
    "\n",
    "def data_norm(name, output = False, save_idx=None):\n",
    "    \"\"\"Load one data example and use log transform\n",
    "    Return array of size (Layers, Molecules, 1)\"\"\"\n",
    "    arr = load_array_3d(name) # arr shape is (111, 100) or (molecules, layers)\n",
    "    if output :\n",
    "        arr = arr[-1]\n",
    "    if save_idx is not None:\n",
    "        arr = arr[save_idx,:]\n",
    "    arr = np.log10(arr)[:,:96] # logarithm \n",
    "    arr = arr.reshape((64,96,1))\n",
    "    arr = preprocessing_function(arr) # logarithm \n",
    "    return arr\n",
    "\n",
    "\n",
    "def load_data(data_dir, output=False, save_idx=None):\n",
    "    \"\"\"Load all data (Train/Test) and apply log transform\n",
    "    Return it as np array of size (number_of_examples, Layers, Molecules,1)\"\"\"\n",
    "    data_list = np.sort(glob.glob(f'{data_dir}/*npy'))\n",
    "    array = []\n",
    "    _ = [array.append(data_norm(name, output=output, save_idx=save_idx)) for name in data_list]\n",
    "    array = np.array(array)\n",
    "    return array, data_list\n",
    "\n",
    "\n",
    "def split_data(input_data, output_data):\n",
    "    \"\"\"Split dataset into train/test/validation sets, assuming 80/10/10 split\"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input_data, output_data,\n",
    "                                                        test_size=0.2, random_state=42)    \n",
    "    X_test, X_val, y_test, y_val  = train_test_split(X_test, y_test, test_size=0.5,random_state=42) \n",
    "    return X_train, X_test, X_val, y_train, y_test, y_val\n",
    "\n",
    "\n",
    "def load_names(X_dir):\n",
    "    data_list_X = np.sort(glob.glob(f'{X_dir}/*npy'))\n",
    "    file_names = [path.split('/')[-1] for path in data_list_X]\n",
    "    X_train, X_test, X_val, _, _, _ = split_data(file_names, file_names)\n",
    "    return X_train, X_test, X_val\n",
    "\n",
    "\n",
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, filenames, batch_size, save_idx, configuration):\n",
    "        self.filenames = filenames\n",
    "        self.configuration = configuration\n",
    "        self.batch_size = batch_size\n",
    "        self.save_idx = save_idx\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.filenames) / self.batch_size))\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        np.random.shuffle(self.filenames)\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_filenames = self.filenames[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_X = []\n",
    "        batch_y = []\n",
    "        batch_info =[]\n",
    "        for filename in batch_filenames:\n",
    "            input_data = data_norm(f'/../{filename}',  save_idx = self.save_idx)\n",
    "            output_data = data_norm(f'/../{filename}', output=True, save_idx = self.save_idx)  # Update with your output data\n",
    "            additional_info = self.configuration[self.configuration['System_ID'].isin([filename.split('.')[0]])]\n",
    "            additional_info = list(additional_info[['planet_radius', 'planet_mass', 'co_ratio','isothermal_T', 'metalicity']].to_numpy()[0])\n",
    "            batch_X.append(input_data)\n",
    "            batch_y.append(output_data)\n",
    "            batch_info.append(additional_info)\n",
    "        batch_X = np.array(batch_X)\n",
    "        batch_y = np.array(batch_y)\n",
    "        batch_info = np.array(batch_info)\n",
    "        return [batch_X,batch_info],  batch_y\n",
    "\n",
    "# Network\n",
    "# =============================================================================\n",
    "def activation_function(conv, act_type='lrelu', name='activation'):\n",
    "    if act_type == 'lrelu':\n",
    "        result = LeakyReLU(alpha=0.2,name=f'{act_type}_{name}')(conv)\n",
    "        return result\n",
    "        \n",
    "    if act_type == 'prelu':\n",
    "        result = PReLU(name=f'{act_type}_{name}')(conv)\n",
    "        return result\n",
    "        \n",
    "    if act_type == 'elu':\n",
    "        result = ELU(name=f'{act_type}_{name}')(conv)\n",
    "        return result\n",
    "    \n",
    "    \n",
    "def model_additional_info(input_dim=5, output_shape=(8,12,1)):\n",
    "    \"\"\"\n",
    "    Creates a neural network model that takes a vector of specified input dimensions\n",
    "    and outputs a map of specified shape.\n",
    "\n",
    "    Parameters:\n",
    "    input_dim (int): Number of variables in the input vector.\n",
    "    output_shape (tuple): Dimensions of the output map.\n",
    "\n",
    "    Returns:\n",
    "    model: A Keras Sequential model.\n",
    "    \"\"\"\n",
    "\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dense, Reshape\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(32, activation='linear', input_dim=input_dim))\n",
    "    model.add(LeakyReLU(alpha=0.2))  # First layer\n",
    "    model.add(Dense(64, activation='linear'))\n",
    "    model.add(LeakyReLU(alpha=0.2))                       # Second layer\n",
    "    model.add(Dense(output_shape[0] * output_shape[1], activation='linear'))\n",
    "    model.add(LeakyReLU(alpha=0.2))  # Output layer\n",
    "    model.add(Reshape(output_shape))                              # Reshaping output\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def convolution_module(input_tensor, filters,num, drop_rate = 0.2):\n",
    "    conv = Conv2D(filters, (3, 3), padding='same', name = f'convolution_{num}_2')(input_tensor)\n",
    "    conv = LeakyReLU(alpha=0.2, name=f'lrelu_{num}')(conv)\n",
    "    conv = Conv2D(filters, (3, 3), padding='same', name = f'convolution_{num}_2')(conv)\n",
    "    conv = LeakyReLU(alpha=0.2, name=f'lrelu_{num}_2')(conv)\n",
    "    pool = MaxPooling2D((2, 2), padding='same', name =f'pool{num}')(conv)\n",
    "    return pool, conv\n",
    "\n",
    "\n",
    "def AttnBlock2D(x, g, desired_dimensionality, num):\n",
    "    x_shape = x.shape\n",
    "    g_shape = g.shape\n",
    "    xl = Conv2D(desired_dimensionality,(1,1),strides=(2,2),\n",
    "                activation=\"relu\",padding = \"same\", name=f'Attention_conv_x_{num}')(x)\n",
    "    gl = Conv2D(desired_dimensionality,(1,1),\n",
    "                activation=\"relu\",padding = \"same\", name=f'Attention_conv_g_{num}')(g)\n",
    "    xg = Add(name=f'Attention_add_{num}')([xl,gl])\n",
    "    xg = Activation(\"relu\", name=f'Attention_relu_{num}')(xg)\n",
    "    xg = Conv2D(1,(1,1),activation=\"sigmoid\",padding = \"same\", name=f'Attention_conv_{num}')(xg)\n",
    "    xg_shape = xg.shape\n",
    "    xg = UpSampling2D((x_shape[1]//xg_shape[1],x_shape[2]//xg_shape[2]), \n",
    "                                      name=f'Attention_up_{num}')(xg)\n",
    "    output = Multiply(name=f'Attention_mult_{num}')([xg,x])\n",
    "    return output\n",
    "\n",
    "\n",
    "def attention_upsample_and_concat(x1, x2, output_channels, in_channels, filters,num,drop_rate = 0.2):\n",
    "    pool_size = 2\n",
    "    x2 = AttnBlock2D(x2, x1, in_channels, num)\n",
    "    upsampled = Conv2DTranspose(output_channels, (pool_size, pool_size),\n",
    "                                strides=(pool_size, pool_size), padding='same', name=f'upsample_{num}')(x1)\n",
    "    concat = Concatenate(axis=3)([upsampled, x2])\n",
    "    conv = Conv2D(filters, (3, 3), padding='same', name =f'convolution_{num}')(concat)\n",
    "    conv = activation_function(conv, act_type=activation, name=f'{num}_up') \n",
    "    \n",
    "    conv = Conv2D(filters, (3, 3), padding='same',name=f'convolution_{num}_2' )(conv)\n",
    "    conv = activation_function(conv, act_type=activation,name= f'{num}_2_up')  \n",
    "    return conv\n",
    "\n",
    "\n",
    "def network(input, additional_input, additional_output_shape = (8, 12, 1)):\n",
    "    pool1, conv1 = convolution_module(input, 32,1)\n",
    "    pool2, conv2 = convolution_module(pool1, 64,2)\n",
    "    pool3, conv3 = convolution_module(pool2, 128,3)\n",
    "    pool4, conv4 = convolution_module(pool3, 256,4)\n",
    "\n",
    "    additional_model = model_additional_info(output_shape = additional_output_shape)\n",
    "    additional_output = additional_model(additional_input)\n",
    "    \n",
    "    combined = concatenate([conv4, additional_output])\n",
    "    \n",
    "    up7 = attention_upsample_and_concat(combined, conv3, 128, 257,128,7)\n",
    "    up8 = attention_upsample_and_concat(up7, conv2, 64, 128,64,8)\n",
    "    up9 = attention_upsample_and_concat(up8, conv1, 32, 64,32,9)\n",
    "    conv10 = Conv2D(1, (1, 1), padding='same', name = 'convolution_10')(up9)\n",
    "    return conv10\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Callbacks\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "\n",
    "class WandbCustomCallback(Callback):\n",
    "    def __init__(self, val_generator, val_steps, molecules):\n",
    "        super().__init__()\n",
    "        self.val_generator = val_generator\n",
    "        self.val_steps = val_steps  # This will be 1 if you want to use one batch\n",
    "        self.molecules = molecules\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Collect predictions and true labels from the validation generator\n",
    "        # Since val_steps is 1, we only take one batch\n",
    "        val_x, val_y = next(iter(self.val_generator))\n",
    "        val_pred = self.model.predict(val_x)\n",
    "\n",
    "        # Log the plots using the collected data\n",
    "        self.log_prediction_plots(val_x,val_y, val_pred)\n",
    "        self.log_prediction_plots_molecules(val_x,val_y, val_pred, self.molecules)\n",
    "        self.height_abundance_plot(val_x,val_y, val_pred, self.molecules)\n",
    "\n",
    "    def log_prediction_plots(self,val_x, true_data, pred_data):\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "       \n",
    "        ax.plot(true_data[:4,:,:].flatten(), pred_data[:4,:,:].flatten(), '.', color='royalblue')\n",
    "        ax.plot([-1, 1], [-1, 1], '-', color='Black')\n",
    "        plt.xlabel('Ground truth')\n",
    "        plt.ylabel('Prediction')\n",
    "        plt.title('All layers and molecules in 4 validation atmospheres')\n",
    "        plt.grid()\n",
    "        plt.tight_layout()\n",
    "        wandb.log({\"Predicted vs True - 4 Atmospheres\": wandb.Image(plt)})\n",
    "        plt.close()\n",
    "\n",
    "\n",
    "    def log_prediction_plots_molecules(self,val_x, true_data, pred_data, molecules):\n",
    "        molecules_list = [40, 43, 6, 45, 38, 13, 35, 53, 31, 46]\n",
    "        for mol in molecules_list:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(6, 5))\n",
    "            ax.plot(true_data[:4, mol, :].flatten(), pred_data[:4, mol, :].flatten(), '.', color='royalblue')\n",
    "            ax.plot([-1, 1], [-1, 1], '-', color='Black')\n",
    "            \n",
    "            plt.xlabel('Ground truth')\n",
    "            plt.ylabel('Prediction')\n",
    "            plt.title(f'All layers for {molecules[mol]} in 4 validation atmospheres')\n",
    "            plt.grid()\n",
    "            plt.tight_layout()\n",
    "            wandb.log({f\"Predicted vs True - Molecule {molecules[mol]}\": wandb.Image(plt)})\n",
    "            plt.close()\n",
    "\n",
    "    def height_abundance_plot(self,val_x, true_data, pred_data, molecules):\n",
    "        molecules_list = ['CH3', 'CH4', 'O2', 'CO', 'CO2', 'C2H5', 'C2H2', 'C2H4', 'H2O']\n",
    "        layers = np.arange(0, 64)\n",
    "        colors = [\"#e6194B\", \"#3cb44b\", \"#ffe119\", \"#4363d8\", \"#f58231\", \n",
    "                  \"#911eb4\", \"#42d4f4\", \"#f032e6\", \"#bfef45\"]\n",
    "        for chosen_atm in [0, 1, 2, 3]:\n",
    "            fig, ax = plt.subplots(figsize=(9, 4.8))\n",
    "            for i, mol in enumerate(molecules_list):\n",
    "                color = colors[i % len(colors)]\n",
    "                \n",
    "                ax.plot(self.inverse_preprocessing(true_data[chosen_atm, molecules.index(mol), :]),\n",
    "                        layers, '-', label=mol, color=color)\n",
    "                ax.plot(self.inverse_preprocessing(pred_data[chosen_atm, molecules.index(mol), :]),\n",
    "                        layers, '--', label=f'{mol} P', color=color)\n",
    "            plt.grid()\n",
    "            plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            plt.title(f'Predicted (P, dashed line) vs Ground Truth (full line) abundances, atm {chosen_atm}')\n",
    "            plt.xlabel('Abundance')\n",
    "            plt.ylabel('Layer')\n",
    "            plt.tight_layout()\n",
    "            wandb.log({f\"Abundance vs Layer - Atmosphere {chosen_atm}\": wandb.Image(plt)})\n",
    "            plt.close()\n",
    "\n",
    "    def inverse_preprocessing(self, scaled_data):\n",
    "        min_value = -20\n",
    "        max_value = 0\n",
    "        log_data = ((scaled_data + 1) / 2) * (max_value - min_value) + min_value\n",
    "        return log_data\n",
    "\n",
    "# =============================================================================\n",
    "\n",
    "\n",
    "# Data Loading\n",
    "# =============================================================================\n",
    "\n",
    "configuration = pd.read_csv('../data/configuration/system_configuration.csv')\n",
    "\n",
    "par_min = 0.4\n",
    "par_max = 1.5\n",
    "configuration['co_ratio']=(configuration['co_ratio']-par_min)/(par_max-par_min) * 2 - 1\n",
    "\n",
    "par_min = 0.3370963034022992 \n",
    "par_max = 3.876389678499647\n",
    "configuration['planet_radius']=(configuration['planet_radius']-par_min)/(par_max-par_min) * 2 - 1\n",
    "\n",
    "par_min = 0.3\n",
    "par_max = 10\n",
    "configuration['planet_mass']=(configuration['planet_mass']-par_min)/(par_max-par_min) * 2 - 1\n",
    "\n",
    "par_min = 1100\n",
    "par_max = 2000\n",
    "configuration['isothermal_T']=(configuration['isothermal_T']-par_min)/(par_max-par_min) * 2 - 1\n",
    "\n",
    "par_min = 0.5\n",
    "par_max = 100\n",
    "configuration['metalicity']=(configuration['metalicity']-par_min)/(par_max-par_min) * 2 - 1\n",
    "\n",
    "save_molecules = np.array([ 34,  36,  39,  41,  42,  43,  44,  45,  46,  47,  48,  49,  50,\n",
    "        51,  52,  53,  54,  55,  56,  57,  58,  59,  60,  61,  62,  63,\n",
    "        64,  65,  66,  67,  68,  69,  70,  71,  72,  73,  74,  75,  76,\n",
    "        77,  78,  79,  80,  81,  82,  83,  84,  85,  86,  87,  90,  91,\n",
    "        94,  96,  97, 100, 101, 102, 104, 105, 106, 108, 109, 110])\n",
    "\n",
    "molecule_len = len(save_molecules)\n",
    "\n",
    "molecules= [\"HCNO\",\"N2O\",\"CH2CHO\",\"CH3CO\",\"NCO\",\"CH3O\",\"O2\",\"CH3CHO\",\"HNO\",\"C\",\n",
    "            \"CHCO\",\"CO2H\",\"HOCN\",\"C2H5\",\"C2H\",\"CH2OH\",\"CH\",\"C2H6\",\"C2H3\",\"CH2CO\",\"NNH\",\n",
    "            \"H2CN\",\"CH3OH\",\"N4S\",\"N2D\",\"CN\",\"1CH2\",\"HNCO\",\"NO\",\"O3P\",\"O1D\",\"C2H4\",\"NH\",\n",
    "            \"3CH2\",\"HCO\",\"C2H2\",\"H2CO\",\"NH2\",\"CO2\",\"OH\",\"CH3\",\"HCN\",\"NH3\",\"CH4\",\"N2\",\n",
    "            \"CO\",\"H2O\",\"H\",\"He\",\"H2\",\"N2H2\",\"N2H3\",\"HNOH\",\"NH2OH\",\"H2NO\",\"C2N2\",\"HCNH\",\n",
    "            \"HNC\",\"NCN\",\"HCOH\",\"HOCHO\",\"H2Oc\",\"CH4c\",\"NH3c\"]\n",
    "\n",
    "X_train, X_test, X_val = load_names('/../data/ode_results_time_steps/')\n",
    "train_generator = DataGenerator(X_train, batch_size, save_molecules,configuration)\n",
    "test_generator = DataGenerator(X_test , batch_size, save_molecules,configuration)\n",
    "val_generator = DataGenerator(X_val, batch_size, save_molecules,configuration)\n",
    "\n",
    "# =============================================================================\n",
    "            \n",
    "\n",
    "input_shape = (64, 64, 1)\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "additional_input_shape = (5,)\n",
    "additional_input = tf.keras.Input(shape=additional_input_shape)\n",
    "\n",
    "output = network(inputs,additional_input)\n",
    "model = Model(inputs=[inputs,additional_input], outputs=output)\n",
    "wandb_custom_callback = WandbCustomCallback(val_generator, 2, molecules)\n",
    "\n",
    "callbacks = [WandbMetricsLogger(), wandb_custom_callback,\n",
    "    ModelCheckpoint(filepath=f'{version_name}/{model_version}.h5', monitor='val_loss',\n",
    "                    save_best_only=True),\n",
    "    TensorBoard(log_dir='logs'),\n",
    "    EarlyStopping(monitor='val_loss', patience=7),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=4, min_lr=0.2*0.2*learning_rate_value),\n",
    "    CSVLogger(f'{version_name}/training_log.csv')]\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=learning_rate_value), loss='mean_absolute_error' )\n",
    "model.fit(train_generator, epochs=epoch_num, validation_data=val_generator,callbacks=callbacks, steps_per_epoch=steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fbb5ae",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m additional_input_shape \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m5\u001b[39m,)\n\u001b[1;32m      4\u001b[0m additional_input \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39madditional_input_shape)\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mnetwork\u001b[49m(inputs,additional_input)\n\u001b[1;32m      7\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39m[inputs,additional_input], outputs\u001b[38;5;241m=\u001b[39moutput)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'network' is not defined"
     ]
    }
   ],
   "source": [
    "input_shape = (64, 96, 1)\n",
    "inputs = tf.keras.Input(shape=input_shape)\n",
    "additional_input_shape = (5,)\n",
    "additional_input = tf.keras.Input(shape=additional_input_shape)\n",
    "\n",
    "output = network(inputs,additional_input)\n",
    "model = Model(inputs=[inputs,additional_input], outputs=output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e2bd8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d47753",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
